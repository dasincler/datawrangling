{

 "metadata": {

  "name": "",

  "signature": "sha256:4fa1dce69b9fc4718b19401ad96b382636dbd0b69e7a3d7eaf158ee5cf16acec"

 },

 "nbformat": 3,

 "nbformat_minor": 0,

 "worksheets": [

  {

   "cells": [

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "#Data Wrangling with MongoDB Project\n",

      "Author: Dirk Sincler  \n",

      "10/28/2015\n",

      "\n",

      "This project is for completion of the [Data Wrangling with MongoDB](https://www.udacity.com/course/data-wrangling-with-mongodb--ud032) component of the [Udacity Data Analysis Nanodegree](https://www.udacity.com/course/data-analyst-nanodegree--nd002).  In this notebook I'll demonstrate methods for parsing XML documents to extract data, auditing and cleaning fields, shaping data for storage, and finally running queries in MongoDB.\n",

      "\n",

      "Below are the modules utilized for this work, in this workflow it is easiest to load them first for have them on tap for the rest of the notebook."

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "import xml.etree.cElementTree as ET #parse XML\n",

      "import pprint #pretty print\n",

      "import re #regular expressions\n",

      "from collections import defaultdict #specialized containers\n",

      "import codecs #encoding/decoding\n",

      "import json #encoding/decoding\n",

      "import pymongo #mongodb api\n",

      "import os #os interfaces\n",

      "\n",

      "path = \"C:\\Users\\coreysauce\\Desktop\\MTAData\\portland_oregon_sampled.osm\""

     ],

     "language": "python",

     "metadata": {},

     "outputs": [],

     "prompt_number": 6

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "\n",

      "***\n",

      "##1. Exploring the Dataset\n",

      "\n",

      "The dataset we'll be working with is an extract of the XML data for [Portland, Oregon](https://www.openstreetmap.org/relation/186579#map=11/45.5431/-122.6545 from Open Street) from Open Street Maps.  I live in Portland so I thought it would be fun to work with a local dataset.  Also, my knowledge of the local idiosyncracies could come in useful in spotting problems in the dataset, such as in address data, for example.  I pulled the XML OSM file from [MapZen's](https://mapzen.com/data/metro-extracts) collection of pre-extracted metro data.  Because the 77MB compressed file extracted to over 1.4G, I sampled the file to bring it down to a more managable size for my machine with the following:"

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",

      "    \"\"\"Yield element if it is the right type of tag\n",

      "\n",

      "    Reference:\n",

      "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",

      "    \"\"\"\n",

      "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",

      "    _, root = next(context)\n",

      "    for event, elem in context:\n",

      "        if event == 'end' and elem.tag in tags:\n",

      "            yield elem\n",

      "            root.clear()\n",

      "\n",

      "\n",

      "with open(SAMPLE_FILE, 'wb') as output:\n",

      "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",

      "    output.write('<osm>\\n  ')\n",

      "\n",

      "    # Write every 10th top level element\n",

      "    for i, element in enumerate(get_element(OSM_FILE)):\n",

      "        if i % 10 == 0:\n",

      "            output.write(ET.tostring(element, encoding='utf-8'))\n",

      "\n",

      "    output.write('</osm>')"

     ],

     "language": "python",

     "metadata": {},

     "outputs": []

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "The resulting file size and sample of the contents of the XML are below."

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "os.path.getsize(path)"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "metadata": {},

       "output_type": "pyout",

       "prompt_number": 10,

       "text": [

        "147651161L"

       ]

      }

     ],

     "prompt_number": 10

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "with open(path, \"r\") as datafile:\n",

      "    preview = datafile.read()\n",

      "    \n",

      "print preview[0:1000]"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",

        "<osm>\n",

        "  <node changeset=\"7632877\" id=\"27195852\" lat=\"45.5408932\" lon=\"-122.8675556\" timestamp=\"2011-03-21T23:25:58Z\" uid=\"393906\" user=\"Grant Humphries\" version=\"11\">\n",

        "\t\t<tag k=\"highway\" v=\"traffic_signals\" />\n",

        "\t</node>\n",

        "\t<node changeset=\"9785170\" id=\"27200095\" lat=\"45.5059342\" lon=\"-122.766918\" timestamp=\"2011-11-09T21:50:47Z\" uid=\"362111\" user=\"Mele Sax-Barnett\" version=\"3\" />\n",

        "\t<node changeset=\"11492907\" id=\"27266260\" lat=\"45.537931\" lon=\"-122.900224\" timestamp=\"2012-05-03T21:09:18Z\" uid=\"362111\" user=\"Mele Sax-Barnett\" version=\"7\" />\n",

        "\t<node changeset=\"14506362\" id=\"27286345\" lat=\"45.5285967\" lon=\"-122.8890331\" timestamp=\"2013-01-02T22:23:51Z\" uid=\"393906\" user=\"Grant Humphries\" version=\"8\" />\n",

        "\t<node changeset=\"7632877\" id=\"27287658\" lat=\"45.5409169\" lon=\"-122.8666457\" timestamp=\"2011-03-21T23:25:59Z\" uid=\"393906\" user=\"Grant Humphries\" version=\"5\" />\n",

        "\t<node changeset=\"9726985\" id=\"27295030\" lat=\"45.5418012\" lon=\"-122.8683008\" timestamp=\"2011-11-03\n"

       ]

      }

     ],

     "prompt_number": 6

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      " *** \n",

      "  \n",

      "To explore the elements of the XML I've used the [Element Tree XML API](https://docs.python.org/2/library/xml.etree.elementtree.html) which allows you to iteratively parse large XML documents without loading the entire file into memory.  Because the file is too large to load into memory to explore, counts are computed for each type of element to get a better sense of the XML structure.\n",

      "\n"

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def count_tags(filename):\n",

      "    tags = {}\n",

      "    for event, elem in ET.iterparse(filename):\n",

      "        if elem.tag not in tags:\n",

      "            tags[elem.tag] = 1\n",

      "        else:\n",

      "            tags[elem.tag] += 1\n",

      "    return tags\n",

      "\n",

      "count_tags(path)"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "metadata": {},

       "output_type": "pyout",

       "prompt_number": 4,

       "text": [

        "{'member': 5510,\n",

        " 'nd': 696304,\n",

        " 'node': 603640,\n",

        " 'osm': 1,\n",

        " 'relation': 579,\n",

        " 'tag': 466045,\n",

        " 'way': 77477}"

       ]

      }

     ],

     "prompt_number": 4

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "It is also necesary to do some exploratory work on some of the fields to get a sense of the kind of cleaning that will need to be done before getting the data into shape.  Below regular expressions are used to search the elements' attributes (what will be the dataset's key values) to make sure we have clean keys in constructing the dataset.  I've returned the keys that have problem characters (defined in problechars variable) and those with characters to don't match any of the regular expressions to see what additional cleaning needs to be done.  Below that is a count of the classification each key value was given.  (output limited)"

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "lower = re.compile(r'^([a-z]|_)*$')\n",

      "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",

      "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",

      "probsother = []\n",

      "\n",

      "def key_type(element, keys):  \n",

      "    if element.tag == \"tag\":\n",

      "        if lower.search(element.attrib['k']):\n",

      "            keys['lower'] += 1\n",

      "        elif lower_colon.search(element.attrib['k']):\n",

      "            keys['lower_colon'] += 1 \n",

      "        elif problemchars.search(element.attrib['k']):\n",

      "            keys['problemchars'] += 1\n",

      "            probsother.append( \"Problem Characters:   \" + element.attrib['k'])\n",

      "        else:\n",

      "            keys['other'] += 1  \n",

      "            probsother.append(\"Other:   \" + element.attrib['k'])\n",

      "        \n",

      "    return keys\n",

      "\n",

      "\n",

      "def process_map(filename): #iteratively parse the document \n",

      "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",

      "    for _, element in ET.iterparse(filename):\n",

      "        keys = key_type(element, keys)\n",

      "\n",

      "    return keys\n",

      "\n",

      "\n",

      "\n",

      "keys = process_map(path)\n",

      "pprint.pprint(keys)\n",

      "pprint.pprint(probsother[0:20])"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{'lower': 221300, 'lower_colon': 241596, 'other': 3149, 'problemchars': 0}\n",

        "['Other:   gnis:Class',\n",

        " 'Other:   gnis:County',\n",

        " 'Other:   gnis:ST_num',\n",

        " 'Other:   gnis:ST_alpha',\n",

        " 'Other:   gnis:County_num',\n",

        " 'Other:   gnis:Class',\n",

        " 'Other:   gnis:County',\n",

        " 'Other:   gnis:ST_num',\n",

        " 'Other:   gnis:ST_alpha',\n",

        " 'Other:   gnis:County_num',\n",

        " 'Other:   gnis:Class',\n",

        " 'Other:   gnis:County',\n",

        " 'Other:   gnis:ST_num',\n",

        " 'Other:   gnis:ST_alpha',\n",

        " 'Other:   gnis:County_num',\n",

        " 'Other:   gnis:Class',\n",

        " 'Other:   gnis:County',\n",

        " 'Other:   gnis:ST_num',\n",

        " 'Other:   gnis:ST_alpha',\n",

        " 'Other:   gnis:County_num']\n"

       ]

      }

     ],

     "prompt_number": 15

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "The key attributes above look like I would expect so there is no cleaning necessary, just some considerations when transforming the data.    "

     ]

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "##2. Cleaning the XML Data\n",

      "\n",

      "Next we'll look at the street value field to see what will be needed to gain consistency in the street type.  The output of this code below gives a dictionary of the unexpected street keys with the entire street name as values.  From this list we manually add new street key/value pairs to the *mapping* dictionary based on observed variations of street designations.  "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",

      "\n",

      "\n",

      "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \\\n",

      "            \"Trail\", \"Parkway\", \"Commons\", \"Loop\", \"Circle\", \"Run\", \"Highway\", \"Terrace\", \"West\", \\\n",

      "            \"East\", \"North\", \"South\", \"Way\"]\n",

      "\n",

      "mapping = { \"St\": \"Street\", \"St.\": \"Street\", \"Rd.\":\"Road\", \"Rd\":\"Road\", \"Ave.\":\"Avenue\", \"Ave\":\"Avenue\",\\\n",

      "           \"Hwy\": \"Highway\", \"Dr\":\"Drive\", \"W\":\"West\", \"E\":\"East\", \"N\":\"North\", \"S\":\"South\"}\n",

      "            \n",

      "\n",

      "def audit_street_type(street_types, street_name):   #identify street names not contained in expected\n",

      "    m = street_type_re.search(street_name)\n",

      "    if m:\n",

      "        street_type = m.group()\n",

      "        if street_type not in expected:\n",

      "            street_types[street_type].add(street_name)\n",

      "\n",

      "\n",

      "def is_street_name(elem):                           #identify if the attribute contains a street key\n",

      "    return (elem.attrib['k'] == \"addr:street\")\n",

      "\n",

      "\n",

      "def audit(osmfile):                                 #parse and audit tags if element is a node or way\n",

      "    osm_file = open(osmfile, \"r\")\n",

      "    street_types = defaultdict(set)\n",

      "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",

      "\n",

      "        if elem.tag == \"node\" or elem.tag == \"way\":\n",

      "            for tag in elem.iter(\"tag\"):\n",

      "                if is_street_name(tag):\n",

      "                    audit_street_type(street_types, tag.attrib['v'])\n",

      "\n",

      "    return street_types\n",

      "\n",

      "\n",

      "def update_name(name, mapping):                     #update the name of the street type if it is not expected\n",

      "    streetmatch = street_type_re.search(name)\n",

      "    if streetmatch:\n",

      "        streettype = streetmatch.group()\n",

      "        if streettype not in expected and streettype in mapping:        \n",

      "            name = re.sub(street_type_re, mapping[streettype], name)\n",

      "        \n",

      "            \n",

      "    return name\n",

      "\n",

      "\n",

      "st_types = audit(path)\n",

      "pprint.pprint(dict(st_types))\n",

      "\n",

      "    "

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{'101': set(['Northwest Hoyt Street #101']),\n",

        " '211': set(['South Highway 211', 'Southeast Highway 211']),\n",

        " '212': set(['Southeast Highway 212']),\n",

        " '213': set(['Highway 213', 'South Highway 213']),\n",

        " '224': set(['Southeast Highway 224']),\n",

        " '26': set(['Southeast Highway 26']),\n",

        " '47': set(['Northwest Highway 47', 'Southwest Old Highway 47']),\n",

        " '99': set(['Northeast Highway 99']),\n",

        " '99E': set(['South Highway 99E']),\n",

        " '99W': set(['Northeast State Highway 99W', 'Southwest Old Highway 99W']),\n",

        " '99e': set(['South Highway 99e']),\n",

        " '99w': set(['North Highway 99w']),\n",

        " 'Ave': set(['NE 10th Ave',\n",

        "             'NE Cumulus Ave',\n",

        "             'NE Hazel Dell Ave',\n",

        "             'NW Birdsdale Ave',\n",

        "             'Northeast 112th Ave',\n",

        "             'Pacific Ave',\n",

        "             'SW 13th Ave',\n",

        "             'Southeast 172nd Ave']),\n",

        " 'Broadway': set(['North Broadway',\n",

        "                  'Northeast Broadway',\n",

        "                  'Northwest Broadway',\n",

        "                  'Southwest Broadway']),\n",

        " 'Byway': set(['Southwest Kings Byway']),\n",

        " 'Cervantes': set(['Cervantes']),\n",

        " 'Chantilly': set(['Southwest Chantilly']),\n",

        " 'Churchill': set(['Southwest Churchill']),\n",

        " 'Circus': set(['Northwest Luray Circus',\n",

        "                'Southwest Esquiline Circus',\n",

        "                'Southwest Fairview Circus']),\n",

        " 'Crest': set(['Collins Crest', 'Eagle Crest']),\n",

        " 'Douglas': set(['Douglas']),\n",

        " 'Downs': set(['Churchill Downs']),\n",

        " 'Dr': set(['SW Griffith Dr']),\n",

        " 'End': set(['Southeast Roads End', 'Southwest Woody End']),\n",

        " 'Falstaff': set(['Falstaff']),\n",

        " 'Fieldcrest': set(['Southeast Fieldcrest']),\n",

        " 'GLN': set(['Southwest Malcolm GLN']),\n",

        " 'Grotto': set(['The Grotto']),\n",

        " 'Heights': set(['Southeast Belmore Heights']),\n",

        " 'Hwy': set(['Historic Columbia River Hwy']),\n",

        " 'Jamaica': set(['Southwest Jamaica']),\n",

        " 'Miami': set(['Southwest Miami']),\n",

        " 'Point': set(['Southwest East Lake Point']),\n",

        " 'Polonius': set(['Polonius']),\n",

        " 'Preakness': set(['Southwest Preakness']),\n",

        " 'Rd': set(['NW Cornell Rd',\n",

        "            'NW Saltzman Rd',\n",

        "            'SW Bany Rd',\n",

        "            'SW Davis Rd',\n",

        "            'Southwest Kinnaman Rd']),\n",

        " 'Spinosa': set(['Spinosa']),\n",

        " 'St': set(['NE Third St', 'SE Division St', 'SE Stark St']),\n",

        " 'St.': set(['NW 18th St.', 'SW Washington St.']),\n",

        " 'Summit': set(['Nansen Summit']),\n",

        " 'Touchstone': set(['Touchstone']),\n",

        " 'View': set(['Southeast Shannon View', 'Southwest Westwood View']),\n",

        " 'Wheatherstone': set(['Wheatherstone']),\n",

        " 'Wheatland': set(['Southwest Wheatland']),\n",

        " 'Woods': set(['Summer Woods']),\n",

        " 'highway': set(['S Columbia River highway']),\n",

        " 'street': set(['Southwest Baker street', 'southwest jefferson street'])}\n"

       ]

      }

     ],

     "prompt_number": 9

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "Below we can see the result of the *update_name()* function to check the outcome of the cleaning.  Obviously we can't create a mapping for every problem street type, but we an at lease solve the most commonly problems programmatically and could potententially do the rest manually if necessary.  "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "for st_type, ways in st_types.iteritems():\n",

      "        for name in ways:\n",

      "            better_name = update_name(name, mapping)\n",

      "            if name != better_name:\n",

      "                print name, \"=>\", better_name"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "SW Washington St. => SW Washington Street\n",

        "NW 18th St. => NW 18th Street\n",

        "Historic Columbia River Hwy => Historic Columbia River Highway\n",

        "SW Griffith Dr => SW Griffith Drive\n",

        "SE Division St => SE Division Street\n",

        "SE Stark St => SE Stark Street\n",

        "NE Third St => NE Third Street\n",

        "Southwest Kinnaman Rd => Southwest Kinnaman Road\n",

        "NW Saltzman Rd => NW Saltzman Road\n",

        "SW Davis Rd => SW Davis Road\n",

        "NW Cornell Rd => NW Cornell Road\n",

        "SW Bany Rd => SW Bany Road\n",

        "SW 13th Ave => SW 13th Avenue\n",

        "Northeast 112th Ave => Northeast 112th Avenue\n",

        "Southeast 172nd Ave => Southeast 172nd Avenue\n",

        "NE 10th Ave => NE 10th Avenue\n",

        "NW Birdsdale Ave => NW Birdsdale Avenue\n",

        "NE Hazel Dell Ave => NE Hazel Dell Avenue\n",

        "NE Cumulus Ave => NE Cumulus Avenue\n",

        "Pacific Ave => Pacific Avenue\n"

       ]

      }

     ],

     "prompt_number": 10

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "##3. Shape the Data to JSON\n",

      "\n",

      "Once functions to clean the data are ready the data can be put in a more convenient and concise storage format.  JSON is a good option beause it allows a nested data structure and is supported by MongoDB.  A snapshot of the resulting JSON can be seen in the output window.   "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "lower = re.compile(r'^([a-z]|_)*$')\n",

      "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",

      "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",

      "\n",

      "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",

      "\n",

      "\n",

      "def shape_element(element):                             #iterate over first level node and way tags to extract attributes                           \n",

      "    node = {}\n",

      "    if element.tag == \"node\" or element.tag == \"way\" :\n",

      "        node[\"created\"] = {}\n",

      "        node[\"type\"] = element.tag\n",

      "        for attribute in element.attrib:\n",

      "            if attribute in CREATED:\n",

      "                node[\"created\"][attribute] = element.attrib[attribute]\n",

      "            elif attribute == \"lat\" or attribute == \"lon\":\n",

      "                node[\"pos\"] = [float(element.attrib[\"lat\"]), float(element.attrib[\"lon\"])]\n",

      "            else:\n",

      "                node[attribute] = element.attrib[attribute]\n",

      "        \n",

      "        for child in element:                          #iterate over children of first level tags\n",

      "            if 'k' in child.attrib and 'v' in child.attrib:\n",

      "                if problemchars.search(child.attrib['k']): \n",

      "                    next\n",

      "                if lower_colon.search(child.attrib['k']):\n",

      "                    splitattrib = child.attrib['k'].split(\":\")\n",

      "                    if len(splitattrib) > 2:\n",

      "                        next\n",

      "                    elif splitattrib[0] == \"addr\":\n",

      "                        if \"address\" not in node:\n",

      "                            node[\"address\"] = {}\n",

      "                        if splitattrib[1] == \"street\":\n",

      "                            better_name = update_name(child.attrib['v'], mapping)  #update the name before inserting into dict\n",

      "                            node[\"address\"][splitattrib[1]] = better_name\n",

      "                        node[\"address\"][splitattrib[1]] = child.attrib['v']\n",

      "                    else:\n",

      "                        node[splitattrib[1]] = child.attrib['v']\n",

      "                else:  \n",

      "                    node[child.attrib['k']] = child.attrib['v']\n",

      "            \n",

      "            \n",

      "            if child.tag == \"nd\":                              \n",

      "                if \"node_refs\" not in node:\n",

      "                    node[\"node_refs\"] = []\n",

      "                node[\"node_refs\"].append(child.attrib[\"ref\"])\n",

      "        \n",

      "        return node\n",

      "    else:\n",

      "        return None\n",

      "\n",

      "\n",

      "def process_map(file_in, pretty = False):              #process XML and dump to JSON\n",

      "    \n",

      "    file_out = \"{0}.json\".format(file_in)\n",

      "    data = []\n",

      "    with codecs.open(file_out, \"w\") as fo:\n",

      "        for _, element in ET.iterparse(file_in):\n",

      "            el = shape_element(element)\n",

      "            if el:\n",

      "                data.append(el)\n",

      "                if pretty:\n",

      "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",

      "                else:\n",

      "                    fo.write(json.dumps(el) + \"\\n\")\n",

      "    return data[0:10]\n",

      "\n",

      "\n",

      "process_map(path)\n",

      "    \n",

      "    "

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "metadata": {},

       "output_type": "pyout",

       "prompt_number": 18,

       "text": [

        "[{'created': {'changeset': '7632877',\n",

        "   'timestamp': '2011-03-21T23:25:58Z',\n",

        "   'uid': '393906',\n",

        "   'user': 'Grant Humphries',\n",

        "   'version': '11'},\n",

        "  'highway': 'traffic_signals',\n",

        "  'id': '27195852',\n",

        "  'pos': [45.5408932, -122.8675556],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '9785170',\n",

        "   'timestamp': '2011-11-09T21:50:47Z',\n",

        "   'uid': '362111',\n",

        "   'user': 'Mele Sax-Barnett',\n",

        "   'version': '3'},\n",

        "  'id': '27200095',\n",

        "  'pos': [45.5059342, -122.766918],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '11492907',\n",

        "   'timestamp': '2012-05-03T21:09:18Z',\n",

        "   'uid': '362111',\n",

        "   'user': 'Mele Sax-Barnett',\n",

        "   'version': '7'},\n",

        "  'id': '27266260',\n",

        "  'pos': [45.537931, -122.900224],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '14506362',\n",

        "   'timestamp': '2013-01-02T22:23:51Z',\n",

        "   'uid': '393906',\n",

        "   'user': 'Grant Humphries',\n",

        "   'version': '8'},\n",

        "  'id': '27286345',\n",

        "  'pos': [45.5285967, -122.8890331],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '7632877',\n",

        "   'timestamp': '2011-03-21T23:25:59Z',\n",

        "   'uid': '393906',\n",

        "   'user': 'Grant Humphries',\n",

        "   'version': '5'},\n",

        "  'id': '27287658',\n",

        "  'pos': [45.5409169, -122.8666457],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '9726985',\n",

        "   'timestamp': '2011-11-03T00:11:51Z',\n",

        "   'uid': '362111',\n",

        "   'user': 'Mele Sax-Barnett',\n",

        "   'version': '7'},\n",

        "  'highway': 'traffic_signals',\n",

        "  'id': '27295030',\n",

        "  'pos': [45.5418012, -122.8683008],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '14338804',\n",

        "   'timestamp': '2012-12-20T03:29:13Z',\n",

        "   'uid': '393906',\n",

        "   'user': 'Grant Humphries',\n",

        "   'version': '22'},\n",

        "  'highway': 'traffic_signals',\n",

        "  'id': '27526559',\n",

        "  'pos': [45.5163189, -122.7922059],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '740220',\n",

        "   'timestamp': '2009-03-05T17:37:09Z',\n",

        "   'uid': '28145',\n",

        "   'user': 'amillar',\n",

        "   'version': '10'},\n",

        "  'id': '27526570',\n",

        "  'pos': [45.5095144, -122.7745465],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '11077133',\n",

        "   'timestamp': '2012-03-23T18:56:22Z',\n",

        "   'uid': '362111',\n",

        "   'user': 'Mele Sax-Barnett',\n",

        "   'version': '7'},\n",

        "  'id': '27526685',\n",

        "  'pos': [45.5107957, -122.778586],\n",

        "  'type': 'node'},\n",

        " {'created': {'changeset': '9792257',\n",

        "   'timestamp': '2011-11-10T21:02:40Z',\n",

        "   'uid': '362111',\n",

        "   'user': 'Mele Sax-Barnett',\n",

        "   'version': '9'},\n",

        "  'id': '27542714',\n",

        "  'pos': [45.5328931, -122.8476356],\n",

        "  'type': 'node'}]"

       ]

      }

     ],

     "prompt_number": 18

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "##4. Insert into MongoDB and Test Query\n",

      "\n",

      "I used the *mongoimport* tool to bulk import the JSON into [MongoDB](https://www.mongodb.org).  This is a very simple process, running *mongoimport* from the command line with the following:\n",

      "\n",

      "*mongoimport --db citydata --collection portland portland_oregon_sampled.osm.json*\n",

      "\n",

      "From there I'm using the [PyMongo API](https://api.mongodb.org/python/current/) to query the database. The functions below will connect to the DB, process the query pipelines, and return the response. "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def get_db(db_name):\n",

      "    # For local use\n",

      "    from pymongo import MongoClient\n",

      "    client = MongoClient('localhost:27017')\n",

      "    db = client[db_name]\n",

      "    return db\n",

      "\n",

      "def return_query(db, pipeline):\n",

      "    return db.portland.aggregate(pipeline)\n",

      "\n",

      "def print_results():\n",

      "    db = get_db(\"citydata\")\n",

      "    pipeline = make_pipeline()\n",

      "    result = return_query(db, pipeline)\n",

      "\n",

      "    for item in result:\n",

      "        print item"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [],

     "prompt_number": 12

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "Because exploring data in JSON format is memory intensive, it would probably be more efficient to audit fields from the database.  Below is a query with an aggregate command to count unique zip codes, similar to what what we did with the street names previously.  Most of the postal codes look valid, but they give us a good idea of what we could need to transform in the future.  For example if a 7 digit code is required for analysis or they could be run against a list of valid Portland postal codes to single out bad entries.  I was a little curious as to why there were so many area codes for a relatively small town like portland. I spot checked a couple of these codes and found the downoaded map are was much broader than I exprected."

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def make_pipeline():\n",

      "    \n",

      "    pipeline = [{\"$group\": {\"_id\": \"$address.postcode\",  \n",

      "                         \"count\": {\"$sum\": 1}}}\n",

      "               ] \n",

      "            \n",

      "    return pipeline\n",

      "\n",

      "print_results()"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{u'count': 1, u'_id': u'98686'}\n",

        "{u'count': 3, u'_id': u'97378'}\n",

        "{u'count': 6, u'_id': u'97101'}\n",

        "{u'count': 12, u'_id': u'97148'}\n",

        "{u'count': 1, u'_id': u'97206-2633'}\n",

        "{u'count': 1, u'_id': u'Portland, OR 97209'}\n",

        "{u'count': 33, u'_id': u'97002'}\n",

        "{u'count': 1, u'_id': u'97077'}\n",

        "{u'count': 2, u'_id': u'98661'}\n",

        "{u'count': 1, u'_id': u'97266-1041'}\n",

        "{u'count': 1, u'_id': u'97116-1675'}\n",

        "{u'count': 5, u'_id': u'97115'}\n",

        "{u'count': 2, u'_id': u'98660'}\n",

        "{u'count': 1, u'_id': u'97064'}\n",

        "{u'count': 180, u'_id': u'97024'}\n",

        "{u'count': 214, u'_id': u'97201'}\n",

        "{u'count': 546, u'_id': u'97060'}\n",

        "{u'count': 247, u'_id': u'97009'}\n",

        "{u'count': 456, u'_id': u'97015'}\n",

        "{u'count': 7, u'_id': u'97111'}\n",

        "{u'count': 1, u'_id': u'97116-2431'}\n",

        "{u'count': 909, u'_id': u'97266'}\n",

        "{u'count': 95, u'_id': u'97019'}\n",

        "{u'count': 154, u'_id': u'97231'}\n",

        "{u'count': 1, u'_id': u'97109'}\n",

        "{u'count': 1, u'_id': u'98604'}\n",

        "{u'count': 985, u'_id': u'97222'}\n",

        "{u'count': 260, u'_id': u'97210'}\n",

        "{u'count': 364, u'_id': u'97027'}\n",

        "{u'count': 887, u'_id': u'97220'}\n",

        "{u'count': 464, u'_id': u'97218'}\n",

        "{u'count': 1340, u'_id': u'97223'}\n",

        "{u'count': 228, u'_id': u'97232'}\n",

        "{u'count': 409, u'_id': u'97055'}\n",

        "{u'count': 640, u'_id': u'97215'}\n",

        "{u'count': 96, u'_id': u'97022'}\n",

        "{u'count': 1178, u'_id': u'97006'}\n",

        "{u'count': 92, u'_id': u'97106'}\n",

        "{u'count': 443, u'_id': u'97070'}\n",

        "{u'count': 1, u'_id': u'OR 97006'}\n",

        "{u'count': 1354, u'_id': u'97219'}\n",

        "{u'count': 710, u'_id': u'97225'}\n",

        "{u'count': 65, u'_id': u'97119'}\n",

        "{u'count': 1, u'_id': u'98671'}\n",

        "{u'count': 1188, u'_id': u'97123'}\n",

        "{u'count': 121, u'_id': u'97227'}\n",

        "{u'count': 98, u'_id': u'97205'}\n",

        "{u'count': 2, u'_id': u'98682'}\n",

        "{u'count': 701, u'_id': u'97140'}\n",

        "{u'count': 5, u'_id': u'97051'}\n",

        "{u'count': 1734, u'_id': u'97229'}\n",

        "{u'count': 770, u'_id': u'97030'}\n",

        "{u'count': 250, u'_id': u'97023'}\n",

        "{u'count': 1091, u'_id': u'97213'}\n",

        "{u'count': 896, u'_id': u'97203'}\n",

        "{u'count': 448, u'_id': u'97216'}\n",

        "{u'count': 1149, u'_id': u'97211'}\n",

        "{u'count': 11, u'_id': u'98607'}\n",

        "{u'count': 24, u'_id': u'97204'}\n",

        "{u'count': 410, u'_id': u'97089'}\n",

        "{u'count': 782, u'_id': u'97233'}\n",

        "{u'count': 3, u'_id': u'98684'}\n",

        "{u'count': 481, u'_id': u'97239'}\n",

        "{u'count': 1625, u'_id': u'97007'}\n",

        "{u'count': 707, u'_id': u'97035'}\n",

        "{u'count': 612, u'_id': u'97116'}\n",

        "{u'count': 534, u'_id': u'97013'}\n",

        "{u'count': 1767, u'_id': u'97206'}\n",

        "{u'count': 686, u'_id': u'97008'}\n",

        "{u'count': 1120, u'_id': u'97124'}\n",

        "{u'count': 1, u'_id': u'97116-2446'}\n",

        "{u'count': 960, u'_id': u'97068'}\n",

        "{u'count': 10, u'_id': u'97128'}\n",

        "{u'count': 1, u'_id': u'97242'}\n",

        "{u'count': 38, u'_id': u'97042'}\n",

        "{u'count': 9, u'_id': u'98662'}\n",

        "{u'count': 317, u'_id': u'97113'}\n",

        "{u'count': 1148, u'_id': u'97080'}\n",

        "{u'count': 729, u'_id': u'97086'}\n",

        "{u'count': 859, u'_id': u'97212'}\n",

        "{u'count': 178, u'_id': u'97003'}\n",

        "{u'count': 774, u'_id': u'97214'}\n",

        "{u'count': 540, u'_id': u'97005'}\n",

        "{u'count': 636099, u'_id': None}\n",

        "{u'count': 101, u'_id': u'97209'}\n",

        "{u'count': 1, u'_id': u'98662-6413'}\n",

        "{u'count': 18, u'_id': u'97078'}\n",

        "{u'count': 686, u'_id': u'97062'}\n",

        "{u'count': 45, u'_id': u'97132'}\n",

        "{u'count': 1001, u'_id': u'97267'}\n",

        "{u'count': 431, u'_id': u'97221'}\n",

        "{u'count': 1064, u'_id': u'97202'}\n",

        "{u'count': 8, u'_id': u'97056'}\n",

        "{u'count': 681, u'_id': u'97034'}\n",

        "{u'count': 1136, u'_id': u'97217'}\n",

        "{u'count': 10, u'_id': u'97114'}\n",

        "{u'count': 1, u'_id': u'97225-6345'}\n",

        "{u'count': 1656, u'_id': u'97045'}\n",

        "{u'count': 1, u'_id': u'98601'}\n",

        "{u'count': 940, u'_id': u'97224'}\n",

        "{u'count': 3, u'_id': u'97038'}\n",

        "{u'count': 101, u'_id': u'97133'}\n",

        "{u'count': 1, u'_id': u'97116-2428'}\n",

        "{u'count': 842, u'_id': u'97236'}\n",

        "{u'count': 4, u'_id': u'98665'}\n",

        "{u'count': 1009, u'_id': u'97230'}\n",

        "{u'count': 1, u'_id': u'98642'}\n",

        "{u'count': 100, u'_id': u'97004'}\n"

       ]

      }

     ],

     "prompt_number": 13

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "Here are some stats on the dataset.  Below are the top 10 contributors of elements to the map.  "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def make_pipeline():\n",

      "    \n",

      "    pipeline = [{\"$group\": {\"_id\": \"$created.user\",  \n",

      "                         \"count\": {\"$sum\": 1}}},\n",

      "                {\"$sort\": {\"count\": -1}},\n",

      "                {\"$limit\": 10}\n",

      "               ] \n",

      "            \n",

      "    return pipeline\n",

      "\n",

      "print_results()\n"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{u'count': 195055, u'_id': u'Peter Dobratz_pdxbuildings'}\n",

        "{u'count': 191979, u'_id': u'lyzidiamond_imports'}\n",

        "{u'count': 61606, u'_id': u'Mele Sax-Barnett'}\n",

        "{u'count': 43879, u'_id': u'Darrell_pdxbuildings'}\n",

        "{u'count': 35907, u'_id': u'baradam'}\n",

        "{u'count': 32473, u'_id': u'Grant Humphries'}\n",

        "{u'count': 16047, u'_id': u'Peter Dobratz'}\n",

        "{u'count': 11711, u'_id': u'justin_pdxbuildings'}\n",

        "{u'count': 11117, u'_id': u'amillar-osm-import'}\n",

        "{u'count': 8357, u'_id': u'woodpeck_fixbot'}\n"

       ]

      }

     ],

     "prompt_number": 48

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "And an aggregation command of the number of unique users."

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def make_pipeline():\n",

      "    \n",

      "    pipeline = [{\"$group\": {\"_id\": \"$created.user\"}},\n",

      "                {\"$group\": {\"_id\": \"Unique Users\", \n",

      "                            \"count\": {\"$sum\": 1}}}\n",

      "               ]\n",

      "    return pipeline\n",

      "\n",

      "print_results()\n"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{u'count': 629, u'_id': u'Unique Users'}\n"

       ]

      }

     ],

     "prompt_number": 57

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "In the query below I've queried the number of nodes and ways in the dataset.  "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def make_pipeline():\n",

      "    \n",

      "    pipeline = [{\"$match\": {\"$or\": [{\"type\": \"node\"}, {\"type\":\"way\"}]}},\n",

      "                {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\":1}}}\n",

      "               ]\n",

      "               \n",

      "    return pipeline\n",

      "\n",

      "print_results()"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{u'count': 77472, u'_id': u'way'}\n",

        "{u'count': 603482, u'_id': u'node'}\n"

       ]

      }

     ],

     "prompt_number": 63

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "***\n",

      "And finally, I'm a big fan of coffee so I though it would be interesting to see how many coffee shops have been mapped in Portland.  I noticed in scrolling through the data that \"coffee\" was represented in at least two different ways, here queried under the *cuisine* field as *coffee* and *coffee_shop*.  There was also and *amenity* value for *cafe*, which generally serves coffee, but did not contain a *coffee* cuisine field.  This is the kind of of the case for many different kinds of amenities in the map.  A further cleaning project might mean standardizing these fields so they are more easily searchable.  "

     ]

    },

    {

     "cell_type": "code",

     "collapsed": false,

     "input": [

      "def make_pipeline():\n",

      "    \n",

      "    pipeline = [{\"$match\": {\"$or\": [{\"cuisine\": \"coffee\"}, {\"cuisine\":\"coffee_shop\"}]}},\n",

      "                {\"$group\": {\"_id\": \"$cuisine\", \"count\": {\"$sum\":1}}}\n",

      "               ]\n",

      "               \n",

      "    return pipeline\n",

      "\n",

      "print_results()"

     ],

     "language": "python",

     "metadata": {},

     "outputs": [

      {

       "output_type": "stream",

       "stream": "stdout",

       "text": [

        "{u'count': 1, u'_id': u'coffee'}\n",

        "{u'count': 28, u'_id': u'coffee_shop'}\n"

       ]

      }

     ],

     "prompt_number": 71

    },

    {

     "cell_type": "markdown",

     "metadata": {},

     "source": [

      "##5. Conclusions\n",

      "\n",

      "Overall for the fields I audited it appears this a fairly tidy dataset.  Eventhough it is one of the least standarized formats, the corrections to street type inconsistancies yielded minimal changes.  I would expect the need to further clean the dataset would be dependent on what kind of analysis we wanted to run.  If we were, for example, concerned with the consistency of the amenity categories as I mentioned above, it would be fairly straighforward to clean these up as well.  Also if it turned out that the metropolitan area the dataset gave was too broad for our purposes, we could narrow the geographic area by area code or latitude/longitude.\n",

      "\n",

      "Sources:  \n",

      "[Stackoverflow](stackoverflow.com)  \n",

      "Udacity lecture materials \n",

      "Udacity forums  \n",

      "MongoDB Documentation  \n",

      "Python Documentation  \n"

     ]

    }

   ],

   "metadata": {}

  }

 ]

}